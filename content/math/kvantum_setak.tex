\chapter{Quantum walks}

In classical random walks, the walker moves from the current vertex via one of its outgoing edges, chosen randomly, weighted by the edge weights. This random choice can be interpreted as a (generalized) coin toss.

To formulate a quantum version of graph walking, we define the quantum coin, which will replace the classical concept of randomness with quantum superposition.

\section{Formulating the Quantum coin}

I used Renato Portugal's Quantum Walks and Search Algorithms~\cite{Portugal} book as a reference for the different types of coins I present in this section.

A \textit{quantum coin} is a quantum system, which behaves according to the postulates of quantum mechanics. It has a current state, represented by a state vector in a Hilbert space and a unitary time evolution operator, describing a coin toss.

After tossing the coin, the resulting coin state chooses the next step of the quantum walker. If there are $d$ outgoing edges to choose from, then the coin's state space must have $d$ orthonormal basis states, each corresponding to one of the possible edges. If the current state is one of the basis states, then the walker moves in that direction. However, in the quantum world, the coin can also be in a superposition, consisting of multiple basis states. This means that the walker will simultaneously move in all corresponding directions and occupy more than one vertex at the same time, resulting in the walker spreading over the graph in a superposition.

For the $d$ dimensional coin state, the corresponding coin flip operator is a $(d\times{}d)$ dimensional unitary matrix. Based on what the transition operator is, multiple types of coins can be defined. The following ones are typically used in quantum walks.

\subsection{Hadamard coin}

The Hadamard coin is the most commonly used quantum coin. It is defined by the Hadamard-matrix as a transition operator:

\begin{align*}
  \mathbf{H} = \frac{1}{\sqrt{2}}\begin{pmatrix}
      1 & 1  \\
      1 & -1
    \end{pmatrix}.
\end{align*}

If the starting coin state is $\ket{0}$, then flipping the coin once results in the following state:

\begin{align*}
 \mathbf{H}\ket{0} = \frac{1}{\sqrt{2}}\begin{pmatrix}
      1 & 1  \\
      1 & -1
    \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix}
    = \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \frac{1}{\sqrt{2}} \ket{0} + \frac{1}{\sqrt{2}} \ket{1}.
\end{align*}

If we measured the above coin, the probability of measuring $0$ is

\begin{align*}
P(0 \mid \frac{1}{\sqrt{2}} (\ket{0} + \ket{1})) =
\left\lvert\frac{1}{\sqrt{2}}\right\rvert^2 =
\frac{1}{2}.
\end{align*}

Similarly, if the starting coin state is $\ket{1}$, then flipping the coin once results in the following state:

\begin{align*}
   \mathbf{H}\ket{1} = \frac{1}{\sqrt{2}}\begin{pmatrix}
      1 & 1  \\
      1 & -1
    \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix}
    = \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \frac{1}{\sqrt{2}} \ket{0} - \frac{1}{\sqrt{2}} \ket{1}.
\end{align*}

The probability of measuring $1$ here is similarly

\begin{align*}
P(1 \mid \frac{1}{\sqrt{2}} (\ket{0} - \ket{1})) =
\left\lvert-\frac{1}{\sqrt{2}}\right\rvert^2 =
\frac{1}{2}.
\end{align*}

An unexpected feature of this coin comes from the fact, that the Hadamard-matrix is Hermitian (self-adjoint), i.e. $\mathbf{H}^{\dagger} = \mathbf{H}$, while also unitary, i.e. $\mathbf{H}^{\dagger} = \mathbf{H}^{-1}$, which results in $\mathbf{H}^{-1} = \mathbf{H}$, thus $\mathbf{H}\mathbf{H} = \mathbf{I}$. This means, that after flipping the coin twice without measuring it, it will return the coin state to its origin. For example, starting from $\ket{0}$:

\begin{align*}
 \mathbf{H}^2 \ket{0} = \mathbf{H}\frac{1}{\sqrt{2}}(\ket{0} + \ket{1}) = \frac{1}{2}(\ket{0} + \ket{1} + \ket{0} - \ket{1}) = \ket{0}.
\end{align*}

After the second flip, the probability of measuring $\ket{0}$ is $1$, due to the destructive interference between the two $\ket{1}$ probability amplitudes, demonstrating a significant contrast between classical and quantum walks.

\begin{definition}[$2^n$ dimensional Hadamard-coin]

A $2^n$ dimensional Hadamard-coin operator can be created by taking the tensor product of the $2$ dimensional Hadamard-coin $n$ times: $\mathbf{H}^{\otimes{}n}$.

\end{definition}

\subsection{Grover coin}

The Grover coin originates from Grover's search algorithm, where it is applied as the diffusion operator.

Let $\ket{D}$ be the following state:

\begin{align*}
\ket{D} = \mathbf{H}^{\otimes{}n}\ket{0} =
\frac{1}{\sqrt{2^n}} \sum\limits_{i=0}^{2^n-1} \ket{i}.
\end{align*}

Using $\ket{D}$, the Grover coin is the following unitary matrix:

\begin{align*}
    \mathbf{G} = 2\ket{D}\bra{D} - \mathbf{I}.
\end{align*}

If $N = 2^n$, then $\mathbf{G}$ unrolls to the following representation:

\begin{align*}
  \mathbf{G} = \begin{pmatrix}
      \frac{2}{N} - 1 & \frac{2}{N} & \dots  & \frac{2}{N} \\
      \frac{2}{N} & \frac{2}{N} - 1 & \dots  & \frac{2}{N} \\
      \vdots & \vdots & \ddots & \vdots \\
      \frac{2}{N} & \frac{2}{N} & \ddots & \frac{2}{N} - 1
    \end{pmatrix}.
\end{align*}

\subsection{Fourier coin}

In contrast to the Hadamard and Grover coins, the Fourier coin can be of any size, not just a power of 2. A size $N$ Fourier-coin, $F_N$ is defined by the matrix of the Quantum Fourier Transform:

\begin{align*}
\mathbf{F}[k,l] = \frac{1}{\sqrt{N}} \omega^{kl}
\end{align*}

where $\omega$ is the $N$-th root of unity,

\begin{align*}
\omega = e^{\frac{2\pi{}i}{N}}.
\end{align*}

$\mathbf{F}$ unrolls to the following representation:

\begin{align*}
\mathbf{F} = \frac{1}{\sqrt{N}}
\begin{pmatrix}
  1 & 1 & 1 & \dots & 1 \\
  1 & \omega & \omega^2 & \dots & \omega^{N-1} \\
  1 & \omega^2 & \omega^4 & \dots & \omega^{2(N-1)} \\
  \vdots & \vdots & \vdots & \ddots & \vdots \\
  1 & \omega^{N-1} & \omega^{2(N-1)} & \dots & \omega^{(N-1)(N-1)}
\end{pmatrix}
\end{align*}

\section{Quantum walks on the line}

Kempe introduces quantum walks from a physicist's perspective in~\cite{KempeIntroduction} using a particle characterised by its position on the line $\ket{x}$ and its spin state $\ket{s}$.

\subsection{State space}

\lineparagraph{Spin state}

The spin state is in $H_2$ with the basis states spin up and down:

\begin{align*}
\ket{\uparrow} = \ket{0}, \\
\ket{\downarrow} = \ket{1}.
\end{align*}

The spin state vector is then given by:

\begin{align*}
\ket{s} = s_0 \ket{\uparrow} + s_1 \ket{\downarrow}.
\end{align*}

\lineparagraph{Position state}

At the start of the walk the particle is at the origin $\ket{0}$ and the walking lasts for $N$ steps. The position state is in $H_{(2N+1)}$ with the following basis vectors corresponding to the possible positions on the line.

\begin{align*}
\{
\ket{-N},\ket{-(N-1)},\dots,
\ket{-1},\ket{0},\ket{1},\dots,
\ket{N-1},\ket{N}
\}
\end{align*}

I index the basis states using negative numbers to match the labels on the axis.

The position state vector is then given by:

\begin{align*}
    \ket{x} = \sum\limits_{i=-N}^{N}x_i\ket{i}.
\end{align*}

\lineparagraph{Composite state}

The composite state of the system, according to \hyperref[PostulateIV]{[PostulateIV]} is then

\begin{align*}
    \ket{x}\otimes{}\ket{s}.
\end{align*}

\subsection{Evolution}

The particle travels on the line based on its current spin state:
\begin{itemize}
\item If the current spin state is $\ket{0}$, the particle moves to the left, i.e. from position $\ket{i}$ the particle travels to position $\ket{i-1}$.
\item If the current spin state is $\ket{1}$, the particle moves to the right, i.e. from position $\ket{i}$ the particle travels to position $\ket{i+1}$.
\end{itemize}

This step is realised with the unitary matrix $\mathbf{S}$ which operates on the complete state of the system, $\ket{x}\otimes{}\ket{s}$ and is assembled from a left and a right shift operator acting on $\ket{x}$ and another operator for checking $\ket{s}$ compiled using tensor product.

\begin{definition}[Left shift operator]

To move from position $\ket{i}$ to the left ($\ket{i-1}$) the position vector is multiplied with the following $\mathbf{L}$ matrix, containing $1$'s above the diagonal. To keep $\mathbf{S}$ unitary, an unused transition must be added in the lower left corner (see \hyperref[PermutationMatricesTheorem]{Theorem \ref{PermutationMatricesTheorem})}.

\begin{align}
\label{LeftShift}
\mathbf{L} = \ket{N}\bra{-N} + \sum\limits_{i=-(N-1)}^{N} \ket{i-1}\bra{i} =
\left(
    \begin{array}{ccccc}
        0      & 1      & 0      & \cdots & 0      \\
               & \ddots & \ddots & \ddots & \vdots \\
        \vdots &        & \ddots & \ddots & 0      \\
        0      &        &        & \ddots & 1      \\
        1      & 0      & \cdots &        & 0
      \end{array}
\right)
\end{align}

For a given basis vector $\ket{j}$ only one of the summands in $\mathbf{L}$ is non-zero, where $i=j$, resulting in the required shift being performed.

\begin{align*}
\mathbf{L}\ket{j} = \ket{j-1}\bra{j}\ket{j} = \ket{j-1}
\end{align*}

\end{definition}

\begin{definition}[Right shift operator]

To move from position $\ket{i}$ to the right ($\ket{i+1}$) the position vector is multiplied with the following $\mathbf{R}$ matrix, containing $1$'s under the diagonal. To keep $\mathbf{S}$ unitary, an unused transition must be added in the top right corner (see \hyperref[PermutationMatricesTheorem]{Theorem \ref{PermutationMatricesTheorem})}.

\begin{align}
\label{RightShift}
\mathbf{R} = \ket{-N}\bra{N} + \sum\limits_{i=-N}^{N-1} \ket{i+1}\bra{i} =
\left(
    \begin{array}{ccccc}
        0      &        & \cdots & 0      & 1      \\
        1      & \ddots &        &        & 0      \\
        0      & \ddots & \ddots &        & \vdots \\
        \vdots & \ddots & \ddots & \ddots &        \\
        0      & \cdots & 0      & 1      & 0
      \end{array}
\right)
\end{align}

For a given basis vector $\ket{j}$ only one of the summands in $\mathbf{R}$ is non-zero, where $i=j$, resulting in the required shift being performed.

\begin{align*}
\mathbf{R}\ket{j} = \ket{j+1}\bra{j}\ket{j} = \ket{j+1}
\end{align*}

\end{definition}

\lineparagraph{Shift operator}

Using matrixes $L$ and $R$ operating on the position register $\ket{x}$ only, we construct a unitary operator $S$, which operates on the composite state of the system, $\ket{x} \otimes \ket{s}$, executing matrix $\mathbf{L}$ on $\ket{x}$ only when $\ket{s} = \ket{0}$ and matrix $\mathbf{R}$ only when $\ket{s}=\ket{1}$.

\begin{align}
\label{ShiftOperator}
  \mathbf{S} = \mathbf{L}\otimes\ket{0}\bra{0} + \mathbf{R}\otimes\ket{1}\bra{1}
\end{align}

The execution logic is as follows:

\begin{gather*}
    \mathbf{S}(\ket{x}\otimes{}\ket{s}) = \\
    (\mathbf{L}\otimes\ket{0}\bra{0} + \mathbf{R}\otimes\ket{1}\bra{1})(\ket{x}\otimes{}\ket{s}) = \\ (\mathbf{L}\otimes\ket{0}\bra{0})(\ket{x}\otimes{}\ket{s}) + (\mathbf{R}\otimes\ket{1}\bra{1})(\ket{x}\otimes{}\ket{s}) = \dots
\end{gather*}

using \hyperref[TensorMixedProduct]{[TensorMixedProduct]}:

\begin{gather*}
    \dots = \mathbf{L}\ket{x}\otimes(\ket{0}\bra{0}\ket{s}) + \mathbf{R}\ket{x}\otimes(\ket{1}\bra{1}\ket{s}) = \\
    \ket{x-1}\otimes{}s_0\ket{0} + \ket{x+1}\otimes{}s_1\ket{1} = \\
    s_0\ket{x-1, 0} + s_1\ket{x+1,1}.
\end{gather*}

\begin{itemize}
\item If the spin state was $\ket{s} = \ket{0}$ at the beginning, then $s_0=1$ and $s_1=0$, which means that the resulting system state is $\ket{x-1,0}$, which means that the particle shifted to the left, as designed.
\item If the spin state was $\ket{s} = \ket{1}$ at the beginning, then $s_0=0$ and $s_1=1$, which means that the resulting system state is $\ket{x+1,1}$, which means that the particle shifted to the right, also as intended.
\end{itemize}

Furthermore, the spin state can be any mixed state $s_0\ket{0} + s_1\ket{1}$ as well. In this case the particle will shift \textit{both} to the left and to the right, at the same time. When measured, the particle can be found in position $\ket{x-1}$ with probability $|s_0|^2$ and in position $\ket{x+1}$ with probability $|s_1|^2$.

In quantum graph walks, the walker can simultaneously explore multiple parallel paths in the graph, at the same time. With good design, this behaviour can be used to search the graph faster than in classical random graph walks.

\lineparagraph{Coin operator}

To inject the quantum superposition into the walk, the particle's spin state is transformed using any $2$ dimensional unitary matrix between shift operations. The Hadamard, Grover and Fourier coins mentioned earlier are commonly used as coin operators.

For any $C$ operator on the coin register, the unitary transform for the composite system is defined as follows:

\begin{align*}
    \mathbf{\hat{C}} = \mathbf{I} \otimes \mathbf{C}
\end{align*}

since the coin operator does not modify the position register.

\lineparagraph{Evolution operator}

Combining the shift operator and the coin operator together, we obtain the following evolution operator, defining one step of the quantum walk on the line. The step consists of flipping the coin once, then applying the shifting the walker's position accordingly, as follows:

\begin{align*}
    \mathbf{U} = \mathbf{S}\mathbf{\hat{C}} = \mathbf{S}(\mathbf{I}\otimes{}\mathbf{C})
\end{align*}

\subsection{Measurement}

To measure the probability of the particle being at position $\ket{i}$, the projective measurement operator acting on $\ket{x}$ is defined as $\mathbf{P_i} = \ket{i}\bra{i}$, in accordance with \hyperref[PostulateIIIProjective]{[PostulateIIIProjective]}.

Since the coin register need not be measured, we apply the identity operator on it, using $\mathbf{P_i} \otimes \mathbf{I}$ on the complete system to measure the particle's current position.

The probability of finding the particle in position $i$ is:

\begin{gather*}
    P(i | \ket{x}) = \bra{x,s}\mathbf{P_i}\otimes{}\mathbf{I}\ket{x,s} = \dots
\end{gather*}

using \hyperref[TensorMixedProduct]{[TensorMixedProduct]}:

\begin{gather*}
    \dots = \bra{x}\mathbf{P_i}\ket{x}\bra{s}\mathbf{I}\ket{s} =
    \bra{x}\mathbf{P_i}\ket{x}1 =
    \bra{x}\mathbf{P_i}\ket{x} =
    \bra{x}\ket{i}\bra{i}\ket{x} =
    |\bra{i}\ket{x}|^2 =
    |x_i|^2
\end{gather*}

\section{Generalization of Quantum Walks}

After presenting quantum walking on the line, I review and extend two approaches to generalize it in this chapter.

\begin{enumerate}
    \item \textbf{Use multiple two-dimensional coins}: In~\cite{Portugal}, Renato Portugal shows the generalization of quantum walks on a line to a two-dimensional grid, using a method with 2 two-dimensional coins. In this work, I prove how his method reduces to effectively two synchronous independent walks on the $x$ and $y$ axes. Then I improve his technique by generalizing to arbitrarily large dimensional grids in a more memory-efficient way than what would naturally follow from his description.
    \item \textbf{Use a single higher dimensional coin}: In~\cite{Portugal}, Renato Portugal describes the generalization of a quantum walk on a line to an arbitrary undirected graph and gives the necessary condition for creating the unitary transition matrix without proof. In this work, I generalize to directed graphs and give proof of the generalization of the condition using directed graphs.
\end{enumerate}

\subsection{Generalization using multiple independent 2 dimensional coins}

In \cite{Portugal} Renato Portugal defines the following method for Quantum Walking on a 2D grid:

Let the position state of the walker be $\ket{x,y}$ and the two coins $\ket{c_x}$, acting on the $x$ coordinate and $\ket{c_y}$, acting on the $y$ coordinate of the walker.

The shift operator moves the walker on the grid diagonally, according to the current state of the two coins, described by

\begin{align}
\label{Portugal2DShift}
    \mathbf{S}\ket{x,y}\ket{c_x}\ket{c_y} = \ket{x + (-1)^{c_x}, y + (-1)^{c_y}}\ket{c_x}\ket{c_y},
\end{align}

and the coin operator leaves the position state in place while flipping both coins at the same time, described by

\begin{align}
\mathbf{\hat{C}} = \mathbf{I} \otimes \mathbf{C_4} = \mathbf{I} \otimes (\mathbf{C_2} \otimes \mathbf{C_2}).
\end{align}

\lineparagraph{Issues with this method}

In \ref{Portugal2DShift}, we can see how the matrix $\mathbf{S}$ will quickly increase in size, as further dimensions are
added to the equation. In 2D, if the walker takes $N$ steps, the size of $\mathbf{S}$ is

\begin{align*}
(2N+1)^2(2N+1)^22^22^2 = 16(2N+1)^4 = O(N^4).
\end{align*}

To increase the dimension count, one would naturally append more coordinates to the composite position state and add further coins, for example in 3D $\mathbf{S}$ would become

\begin{align*}
    \mathbf{S}\ket{x,y,z}\ket{c_x}\ket{c_y}\ket{c_z} =
    \ket{x + (-1)^{c_x}, y + (-1)^{c_y}, z + (-1)^{c_z}}\ket{c_x}\ket{c_y}\ket{c_z},
\end{align*}

For a dimension count $d$, the size of $\mathbf{S}$ is exponential in $d$.

\begin{align*}
((2N+1)^2)^d(2^2)^d = (4(2N+1)^2)^d = O(N^{2d}).
\end{align*}

\lineparagraph{My improvements}

Since in \ref{Portugal2DShift} the coordinates of the walker are updated independently by the separate coins, I was able to disassemble $\mathbf{S}$ into smaller matrices, using the properties of the tensor product.

To do this, I first needed to define $\mathbf{S}$ in 2D explicitly (as opposed to the implicit definition in \ref{Portugal2DShift}, stating only how $\mathbf{S}$ updates the state of the system). I will be using the matrices $\mathbf{L}$ defined by \ref{LeftShift} and $\mathbf{R}$ defined by \ref{RightShift}.

Notice that $\mathbf{R}$ increases the walker's coordinates on the line, while $\mathbf{L}$ decreases them. This means, that on the $y$ axis $\mathbf{R}$ acts by moving the walker up, and $\mathbf{L}$ acts by moving the walker down.

When the coins are in the state $\ket{c_x,c_y} = \ket{0,0}$, the walker moves up and to the right. This movement is captured by $\mathbf{S}_{0,0}$, acting on the position state:

\begin{align*}
    \mathbf{S}_{0,0} =  \mathbf{R} \otimes \mathbf{R}
\end{align*}

The other $3$ $\mathbf{S}_{c_x,c_y}$ matrices are defined similarly: 

\begin{align*}
    \mathbf{S}_{0,1} =  \mathbf{R} \otimes \mathbf{L}\\
    \mathbf{S}_{1,0} =  \mathbf{L} \otimes \mathbf{R}\\
    \mathbf{S}_{1,1} =  \mathbf{L} \otimes \mathbf{L}
\end{align*}

Then, I assemble $\mathbf{S}$ using $\mathbf{S}_{0,0}$, $\mathbf{S}_{0,1}$, $\mathbf{S}_{1,0}$ and $\mathbf{S}_{1,1}$ the following way: I want
\begin{itemize}
\item $\mathbf{S}_{0,0}$ to act only when $\ket{c_x,c_y} = \ket{0,0}$,
\item $\mathbf{S}_{0,1}$ to act only when $\ket{c_x,c_y} = \ket{0,1}$,
\item $\mathbf{S}_{1,0}$ to act only when $\ket{c_x,c_y} = \ket{1,0}$, and finally
\item $\mathbf{S}_{1,1}$ to act only when $\ket{c_x,c_y} = \ket{1,1}$.
\end{itemize}

Using the same method as in \ref{ShiftOperator} I arrive at:

\begin{align*}
    \mathbf{S} =
    \mathbf{S}_{0,0} \otimes \ket{0,0}\bra{0,0} + \\
    \mathbf{S}_{0,1} \otimes \ket{0,1}\bra{0,1} + \\
    \mathbf{S}_{1,0} \otimes \ket{1,0}\bra{1,0} + \\
    \mathbf{S}_{1,1} \otimes \ket{1,1}\bra{1,1}\phantom{+}
\end{align*}

After substituting the $\mathbf{S}_{c_x,c_y}$ matrices in:

\begin{align*} 
    \mathbf{S} =   
    (\mathbf{R} \otimes \mathbf{R}) \otimes \ket{0,0}\bra{0,0} + \\
    (\mathbf{R} \otimes \mathbf{L}) \otimes \ket{0,1}\bra{0,1} + \\
    (\mathbf{L} \otimes \mathbf{R}) \otimes \ket{1,0}\bra{1,0} + \\
    (\mathbf{L} \otimes \mathbf{L}) \otimes \ket{1,1}\bra{1,1}\phantom{+}
\end{align*}

Let us name the coin state $\ket{0}$ heads and the coin state $\ket{1}$ tails. Then, using the following equalities amd introducing matrices $\mathbf{H}$ and $\mathbf{T}$, as shorthands:

\begin{align*} 
    \ket{0,0}\bra{0,0} = (\ket{0}\bra{0}) \otimes (\ket{0}\bra{0}) = \mathbf{H} \otimes \mathbf{H} \\
    \ket{0,1}\bra{0,1} = (\ket{0}\bra{0}) \otimes (\ket{1}\bra{1}) = \mathbf{H} \otimes \mathbf{T} \\
    \ket{1,0}\bra{1,0} = (\ket{1}\bra{1}) \otimes (\ket{0}\bra{0}) = \mathbf{T} \otimes \mathbf{H} \\
    \ket{1,1}\bra{1,1} = (\ket{1}\bra{1}) \otimes (\ket{1}\bra{1}) = \mathbf{T} \otimes \mathbf{T}
\end{align*}

I arrive at:

\begin{align*} 
    \mathbf{S} =   
    (\mathbf{R} \otimes \mathbf{R}) \otimes (\mathbf{H} \otimes \mathbf{H}) + \\
    (\mathbf{R} \otimes \mathbf{L}) \otimes (\mathbf{H} \otimes \mathbf{T}) + \\
    (\mathbf{L} \otimes \mathbf{R}) \otimes (\mathbf{T} \otimes \mathbf{H}) + \\
    (\mathbf{L} \otimes \mathbf{L}) \otimes (\mathbf{T} \otimes \mathbf{T})\phantom{+}
\end{align*}

Then, using \hyperref[TensorMixedProduct]{[TensorMixedProduct]} I inflate the equation with (appropriately sized) $\mathbf{I}$ matrices:

\begin{align*} 
    \mathbf{S} =   
    ((\mathbf{R} \otimes \mathbf{I})(\mathbf{I} \otimes \mathbf{R})) \otimes ((\mathbf{H} \otimes \mathbf{I}) (\mathbf{I} \otimes \mathbf{H})) + \\
    ((\mathbf{R} \otimes \mathbf{I})(\mathbf{I} \otimes \mathbf{L})) \otimes ((\mathbf{H} \otimes \mathbf{I}) (\mathbf{I} \otimes \mathbf{T})) + \\
    ((\mathbf{L} \otimes \mathbf{I})(\mathbf{I} \otimes \mathbf{R})) \otimes ((\mathbf{T} \otimes \mathbf{I}) (\mathbf{I} \otimes \mathbf{H})) + \\
    ((\mathbf{L} \otimes \mathbf{I})(\mathbf{I} \otimes \mathbf{L})) \otimes ((\mathbf{T} \otimes \mathbf{I}) (\mathbf{I} \otimes \mathbf{T}))\phantom{+}
\end{align*}

At this point, I introduce a few aliases to make the equation more manageable. Notice, how for example $\mathbf{I} \otimes \mathbf{R}$ is acting on the 2D position state, but only updating the $y$ coordinate to move the walker upward. This
gives a way to naturally define $\mathbf{S}_{\text{up}} = \mathbf{I} \otimes \mathbf{R}$, and similarly:

\begin{align*} 
\mathbf{S}_{\text{right}} = \mathbf{R} \otimes \mathbf{I} \\
\mathbf{S}_{\text{left}} = \mathbf{L} \otimes \mathbf{I} \\
\mathbf{S}_{\text{up}} = \mathbf{I} \otimes \mathbf{R} \\
\mathbf{S}_{\text{down}} = \mathbf{I} \otimes \mathbf{L} \\
\end{align*}

At the same time, for example $\mathbf{I} \otimes \mathbf{H}$ is acting on the composite coin state, but only checking if the second coin's state is heads, which is the coin for the $y$ axis. This gives a way to naturally define $\mathbf{H}_y = \mathbf{I} \otimes \mathbf{H}$, and similarly:

\begin{align*} 
\mathbf{H}_x = \mathbf{H} \otimes \mathbf{I} \\
\mathbf{T}_x = \mathbf{T} \otimes \mathbf{I} \\
\mathbf{H}_y = \mathbf{I} \otimes \mathbf{H} \\
\mathbf{T}_y = \mathbf{I} \otimes \mathbf{T} \\
\end{align*}

Substituting all of the aliases:

\begin{align*} 
    \mathbf{S} =   
    (\mathbf{S}_{\text{right}}\mathbf{S}_{\text{up}}) \otimes (\mathbf{H}_x\mathbf{H}_y) + \\
    (\mathbf{S}_{\text{right}}\mathbf{S}_{\text{down}}) \otimes (\mathbf{H}_x\mathbf{T}_y) + \\
    (\mathbf{S}_{\text{left}}\mathbf{S}_{\text{up}}) \otimes (\mathbf{T}_x\mathbf{H}_y) + \\
    (\mathbf{S}_{\text{left}}\mathbf{S}_{\text{down}}) \otimes (\mathbf{T}_x\mathbf{T}_y)\phantom{+}
\end{align*}

Then, using \hyperref[TensorMixedProduct]{[TensorMixedProduct]} again, I arrive at:

\begin{align*} 
    \mathbf{S} =   
    (\mathbf{S}_{\text{right}} \otimes \mathbf{H}_x) (\mathbf{S}_{\text{up}} \otimes \mathbf{H}_y) + \\
    (\mathbf{S}_{\text{right}} \otimes \mathbf{H}_x) (\mathbf{S}_{\text{down}} \otimes \mathbf{T}_y) + \\
    (\mathbf{S}_{\text{left}} \otimes \mathbf{T}_x) (\mathbf{S}_{\text{up}} \otimes \mathbf{H}_y) + \\
    (\mathbf{S}_{\text{left}} \otimes \mathbf{T}_x) (\mathbf{S}_{\text{down}} \otimes \mathbf{T}_y)\phantom{+}
\end{align*}

Then using the distributive property of matrix multiplication with respect to matrix addition I finally arrive at:

\begin{align*} 
    \mathbf{S} =   
    ((\mathbf{S}_{\text{right}} \otimes \mathbf{H}_x) + (\mathbf{S}_{\text{left}} \otimes \mathbf{T}_x))
    ((\mathbf{S}_{\text{up}} \otimes \mathbf{H}_y) + (\mathbf{S}_{\text{down}} \otimes \mathbf{T}_y))
\end{align*}

We can see from the equation above, that $\mathbf{S}$ is actually the product of two shift operators, one only
acting on the $x$ coordinate using the first coin's state, the other acting only on the $y$ coordinate, according to the second
coin's state.

\begin{align*} 
    \mathbf{S}_x = (\mathbf{S}_{\text{right}} \otimes \mathbf{H}_x) + (\mathbf{S}_{\text{left}} \otimes \mathbf{T}_x) \\
    \mathbf{S}_y = (\mathbf{S}_{\text{up}} \otimes \mathbf{H}_y) + (\mathbf{S}_{\text{down}} \otimes \mathbf{T}_y) \\
\end{align*}    

\begin{align*}
    \mathbf{S} = \mathbf{S}_x\mathbf{S}_y
\end{align*}

This proves, that the walk on the 2D grid decomposes into two independent line walks on the axes, since $\mathbf{S}_x$ only touches the $x$ coordinate and the first coin, while $\mathbf{S}_y$ only touches the $y$ coordinate and the second coin and there is no entanglement between the registers of the $x$ and $y$ axes. Using this fact, we can simply simulate two independent quantum walks on the line in parallel, or sequentially, using the same registers, which wastly decreases the memory needs of the algorithm. Running in parallel, the memory needs is now $d(4(2N+1)^2) = O(dN^2)$, or running sequentially $(4(2N+1))^2 = O(N^2)$, however the latter uses $dN$ steps, instead of $N$, and $d$ measurements, instead of $1$.

\subsection{Generalization using a single higher dimensional coin}

Using this method, we generalize quantum walking to arbitrary directed graphs. To do so, we first generalize to regular graphs, then discuss how non-regular graphs can be regularized. This introduction is done following Renato Portugal's book~\cite{Portugal}.

In a $d$-regular graph, the vertices have $d$ neighbours, meaning the walker must choose from $d$ possible directions at every step. Thus the coin is $d$ dimensional. Using this idea as a starting point, we can reverse engineer the generalized walk from the walk on the line by starting from the evolution operator, which assumes nothing about the given graph or coin.

\begin{align*}
    \mathbf{U} = \mathbf{S} \mathbf{\hat{C}} = \mathbf{S} (\mathbf{I} \otimes \mathbf{C})
\end{align*}

In this equation, $\mathbf{C}$ can be any $d$-dimensional unitary matrix. The definition of $\mathbf{S}$ requires more thought, since $\mathbf{S}$ has to encode the graph's structure, while also satisfying \hyperref[PostulateII]{[PostulateII]}.

In 1 dimension, $\mathbf{S}$ was defined the following way:

\begin{align*}
\mathbf{S} = \mathbf{L}\otimes\ket{0}\bra{0} + \mathbf{R}\otimes\ket{1}\bra{1}.
\end{align*}

$\ket{0}\bra{0}$ and $\ket{1}\bra{1}$ were matrices that checked the current state of the coin and ''activated'' the transition $\mathbf{L}$ or $\mathbf{R}$ accordingly. In $d$ dimensions, the coin has $d$ possible states, i.e. ''sides''

\begin{align*}
    \{\ket{0}, \ket{1}, \dots, \ket{d-1}\},
\end{align*}

which means $\mathbf{S}$ will be constructed using $d$ transition matrices, describing the graph's structure

\begin{align*}
    \mathbf{S} = \mathbf{S}_0\ket{0}\bra{0} + \mathbf{S}_1\ket{1}\bra{1} + \dots + \mathbf{S}_{d-1}\ket{d-1}\bra{d-1}.
\end{align*}

In the 1 dimensional case $\mathbf{L}$ and $\mathbf{R}$ described stepping to the left and to the right, which are
the directed edges of the line graph and $\mathbf{L} + \mathbf{R}$ is the adjacency matrix of the line graph. To generalize this, $\mathbf{S}_0 + \mathbf{S}_1 + \dots + \mathbf{S}_{d-1}$ is going to be the adjacency matrix of the $d$-regular graph.

The question is how do we construct the matrices $\mathbf{S}_0, \mathbf{S}_1, \dots, \mathbf{S}_{d-1}$ from a given adjacency matrix of a $d$-regular graph? It turns out, that in order for $\mathbf{S}$ to satisfy \hyperref[PostulateII]{[PostulateII]}, there are strict rules on how these $\mathbf{S}_i$ matrices can be defined.

It seems to be a well-known fact in the literature, that for $d$-regular undirected graphs with a valid edge coloring using $d$ colors, the sides of the coin correspond to the colorsets of the edges. This means, that the $S_i$ adjacency matrix contains all of the edges that have the $i$th color assigned to them, in both directions (i.e. $S_i$ is symmetric). This fact is also mentioned in \cite{Portugal}, however no proof is given in this book or any other books and articles I have found during research.

In the following section, I present a more generalized theorem for directed $d$-regular graphs formulated and proven by me. Then I discuss how the special case of the theorem for undirected graphs gives the edge coloring as a result.

\begin{theorem}
\label{PermutationMatricesTheorem}
Given a coined quantum walk on a directed, $d$-regular graph $G$, in the shift operator of the walk: $\mathbf{S} = \sum\limits_{i=0}^{d-1} \mathbf{S}_i \otimes{} \ket{i}\bra{i}$, the $\mathbf{S}_{i}$ are \textbf{permutation} matrices.
\end{theorem}

\textit{Proof.}

According to \hyperref[PostulateII]{[PostulateII]}, $\mathbf{S}$ must be unitary.

According to \hyperref[Unitary]{[Unitary]} the columns of $\mathbf{S}$ form an orthonormal basis. This means, that the scalar product of any two different columns is $0$.

Let $\mathbf{S}$ be a size $N\times{}N$ matrix, then

\begin{align*}
    (\mathbf{S}\ket{k})^{\dagger}(\mathbf{S}\ket{j}) = 0 \addspace{} \forall{}j\neq{}k,\ 0\leq{}j,k\leq{}N.
\end{align*}

Since both the $\mathbf{S}_i$ matrices and the $\ket{i}\bra{i}$ matrices contain only non-complex, non-negative values, this means that $\mathbf{S}$ contains also only non-complex, non-negative values. Thus the only way the scalar product of two different columns can be $0$ is if the columns don't contain non-zero values in the same row.

From this observation follows, that for each individual $S_i$ matrix, no two different columns can contain non-zero values in the same row. If there were two different columns in $S_i$, that contained non-zero values in the same row, then that would result in $\mathbf{S}_i \otimes \ket{i}\bra{i}$ containing two different columns containing non-zero values in the same row, which would result in $\mathbf{S}$ containing two different columns containing non-zero values in the same row (since no matrices contain negative or complex values), which is a contradiction.

Using \hyperref[Unitary]{[Unitary]}, the rows of $\mathbf{S}$ also form an orthonormal basis. With similar reasoning, it can be proven that for each individual $S_i$ matrix, no two different rows can contain non-zero values in the same column.

From these two observations follows, that each $\mathbf{S}_i$ matrix contains exactly one non-zero value in each row and also each column. Adding the fact, that $\mathbf{S}$ matrix's rows and columns are normalized, means that this non-zero value must always be a $1$, resulting in the $\mathbf{S}_i$ being \textbf{permutation} matrices.

\begin{flushright}
$\square{}.$
\end{flushright}

When this theorem is applied to undirected graphs, the $\mathbf{S}_i$ adjacency matrices can be contructed to be symmetric (since the graph's adjacency matrix is also symmetric) and in this case they correspond to a valid edge coloring using $d$ colors (since if $\mathbf{S}_i[j,k] = 1$, then also $\mathbf{S}[k,j] = 1$ due to symmetry, and the $\{k,j\}$ edge has the color $i$, while no other edges of vertex $j$ or $k$ have the $i$th color, since $\mathbf{S}_i$ is a permutation matrix).

Applying Vizing's theorem to $d$-regular graphs, the graphs can be categorized into two classes \cite{Portugal}:
\begin{itemize}
\item Class 1 $d$-regular graphs: Their edge-chromatic number is $d$. The contruction defined in this section works for these graphs.
\item Class 2 $d$-regular graphs: Their edge-chromatic number is $d+1$. The construction defined in this section doesn't work for these graphs. According to \cite{Portugal}, no quantum walk using position-coin notation can be constructed for these, one must use the arc notation and replace the simple graph by an associated symmetric digraph, whose underlying graph is the original graph. In this case, the walker steps on the arcs of the digraph. The details of this construction are out of scope for this work.
\end{itemize}